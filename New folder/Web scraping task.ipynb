{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc1b5e1-9820-4dd3-89ed-d130a411fc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to Extract_Text_Data.csv\n"
     ]
    }
   ],
   "source": [
    "# (1)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "headings = []\n",
    "for heading in soup.find_all(['h1', 'h2']):\n",
    "    headings.append(['Heading', heading.text.strip()])\n",
    "\n",
    "paragraphs = []\n",
    "for paragraph in soup.find_all('p'):\n",
    "    paragraphs.append(['Paragraph', paragraph.text.strip()])\n",
    "\n",
    "list_items = []\n",
    "for li in soup.find_all('li'):\n",
    "    list_items.append(['List Item', li.text.strip()])\n",
    "all_data = headings + paragraphs + list_items\n",
    "\n",
    "with open('Extract_Text_Data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Type', 'Content']) \n",
    "    writer.writerows(all_data)\n",
    "\n",
    "print(\"Data has been successfully written to Extract_Text_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c07426-ff8d-4e8b-a3c1-3d9757a72c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully extracted and saved to Extract_Table_Data2.csv\n"
     ]
    }
   ],
   "source": [
    "#(2)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table')\n",
    "    if table:\n",
    "        headers = [header.text.strip() for header in table.find_all('th')]\n",
    "        rows = []\n",
    "        for row in table.find_all('tr')[1:]: \n",
    "            cells = [cell.text.strip() for cell in row.find_all('td')]\n",
    "            rows.append(cells)\n",
    "        file_path = \"Extract_Table_Data2.csv\"\n",
    "        \n",
    "        with open(file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(headers)  \n",
    "            writer.writerows(rows)    \n",
    "        \n",
    "        print(f\"Data successfully extracted and saved to {file_path}\")\n",
    "    else:\n",
    "        print(\"No table found on the webpage.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39d90409-4dea-402c-b55e-f06b311d29eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product information successfully extracted and saved to Product_Information.json\n"
     ]
    }
   ],
   "source": [
    "#(3)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cards = soup.find_all('div', class_='product-card') \n",
    "    products = []\n",
    "    for card in cards:\n",
    "        title = card.find('h3').text.strip() if card.find('h3') else \"N/A\"  \n",
    "        price = card.find('span', class_='price').text.strip() if card.find('span', class_='price') else \"N/A\"\n",
    "        stock = card.find('span', class_='availability').text.strip() if card.find('span', class_='availability') else \"N/A\"\n",
    "        button_text = card.find('button').text.strip() if card.find('button') else \"N/A\"\n",
    "        \n",
    "        products.append({\n",
    "            \"Book Title\": title,\n",
    "            \"Price\": price,\n",
    "            \"Stock Availability\": stock,\n",
    "            \"Button Text\": button_text\n",
    "        })\n",
    "    \n",
    "    file_path = \"Product_Information.json\"\n",
    "    with open(file_path, mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(products, file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Product information successfully extracted and saved to {file_path}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f2f6cf5-5306-46b4-863d-482bb65073ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to Form_Details.json\n"
     ]
    }
   ],
   "source": [
    "#(4)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    forms = soup.find_all('form')\n",
    "    form_data = []\n",
    "    for form in forms:\n",
    "        inputs = form.find_all('input')\n",
    "        \n",
    "        for inp in inputs:\n",
    "            field_name = inp.get('name', 'N/A')\n",
    "            input_type = inp.get('type', 'N/A')\n",
    "            default_value = inp.get('value', 'N/A')\n",
    "            \n",
    "            # إضافة التفاصيل في قائمة\n",
    "            form_data.append({\n",
    "                \"Field Name\": field_name,\n",
    "                \"Input Type\": input_type,\n",
    "                \"Default Value\": default_value\n",
    "            })\n",
    "    file_path = \"Form_Details.json\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(form_data, file, indent=4)\n",
    "\n",
    "    print(f\"Data successfully saved to {file_path}\")\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adc77f28-714a-427c-8d8c-b924a54dbec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to Links_and_Videos.json\n"
     ]
    }
   ],
   "source": [
    "#(5)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://www.geeksforgeeks.org/how-to-convert-python-dictionary-to-json/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    links = []\n",
    "    for a in soup.find_all('a')[:5]:  \n",
    "        link = a.get('href')\n",
    "        text = a.text.strip()\n",
    "        links.append({\"Link Text\": text, \"Href\": link})\n",
    "    \n",
    "    videos = []\n",
    "    for iframe in soup.find_all('iframe'):\n",
    "        video_src = iframe.get('src')\n",
    "        videos.append({\"Video Source\": video_src})\n",
    "    \n",
    "    data = {\n",
    "        \"Links\": links,\n",
    "        \"Videos\": videos\n",
    "    }\n",
    "\n",
    "    with open(\"Links_and_Videos.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    print(\"Data successfully saved to Links_and_Videos.json\")\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a22d299-557d-4c38-b117-6b81538d6a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to Featured_Products.json\n"
     ]
    }
   ],
   "source": [
    "#(6)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    featured_products = soup.find_all('div', class_='featured-product')\n",
    "    product_data = []\n",
    "    \n",
    "    for product in featured_products:\n",
    "        product_name = product.find('span', class_='name').text.strip() if product.find('span', class_='name') else \"N/A\"\n",
    "        hidden_price = product.find('span', class_='price', style='display: none;').text.strip() if product.find('span', class_='price', style='display: none;') else \"N/A\"\n",
    "        available_colors = product.find('span', class_='colors').text.strip() if product.find('span', class_='colors') else \"N/A\"\n",
    "        product_id = product.get('data-id', 'N/A')\n",
    "        \n",
    "        product_data.append({\n",
    "            \"Product Name\": product_name,\n",
    "            \"Hidden Price\": hidden_price,\n",
    "            \"Available Colors\": available_colors,\n",
    "            \"Product ID\": product_id\n",
    "        })\n",
    "    \n",
    "    with open(\"Featured_Products.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(product_data, file, indent=4)\n",
    "    \n",
    "    print(\"Data successfully saved to Featured_Products.json\")\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f4e9d-963c-4281-a2a2-9679155feb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c5f4d-fc40-4c1b-a290-148cd6764223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
